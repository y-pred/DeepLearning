{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "u3xzkAZRATvS"
      },
      "outputs": [],
      "source": [
        "faqs = \"\"\"About the Program\n",
        "What is the course fee for  Data Science Mentorship Program (DSMP 2023)\n",
        "The course follows a monthly subscription model where you have to make monthly payments of Rs 799/month.\n",
        "What is the total duration of the course?\n",
        "The total duration of the course is 7 months. So the total course fee becomes 799*7 = Rs 5600(approx.)\n",
        "What is the syllabus of the mentorship program?\n",
        "We will be covering the following modules:\n",
        "Python Fundamentals\n",
        "Python libraries for Data Science\n",
        "Data Analysis\n",
        "SQL for Data Science\n",
        "Maths for Machine Learning\n",
        "ML Algorithms\n",
        "Practical ML\n",
        "MLOPs\n",
        "Case studies\n",
        "You can check the detailed syllabus here - https://learnwith.campusx.in/courses/CampusX-Data-Science-Mentorship-Program-637339afe4b0615a1bbed390\n",
        "Will Deep Learning and NLP be a part of this program?\n",
        "No, NLP and Deep Learning both are not a part of this program’s curriculum.\n",
        "What if I miss a live session? Will I get a recording of the session?\n",
        "Yes all our sessions are recorded, so even if you miss a session you can go back and watch the recording.\n",
        "Where can I find the class schedule?\n",
        "Checkout this google sheet to see month by month time table of the course - https://docs.google.com/spreadsheets/d/16OoTax_A6ORAeCg4emgexhqqPv3noQPYKU7RJ6ArOzk/edit?usp=sharing.\n",
        "What is the time duration of all the live sessions?\n",
        "Roughly, all the sessions last 2 hours.\n",
        "What is the language spoken by the instructor during the sessions?\n",
        "Hinglish\n",
        "How will I be informed about the upcoming class?\n",
        "You will get a mail from our side before every paid session once you become a paid user.\n",
        "Can I do this course if I am from a non-tech background?\n",
        "Yes, absolutely.\n",
        "I am late, can I join the program in the middle?\n",
        "Absolutely, you can join the program anytime.\n",
        "If I join/pay in the middle, will I be able to see all the past lectures?\n",
        "Yes, once you make the payment you will be able to see all the past content in your dashboard.\n",
        "Where do I have to submit the task?\n",
        "You don’t have to submit the task. We will provide you with the solutions, you have to self evaluate the task yourself.\n",
        "Will we do case studies in the program?\n",
        "Yes.\n",
        "Where can we contact you?\n",
        "You can mail us at nitish.campusx@gmail.com\n",
        "Payment/Registration related questions\n",
        "Where do we have to make our payments? Your YouTube channel or website?\n",
        "You have to make all your monthly payments on our website. Here is the link for our website - https://learnwith.campusx.in/\n",
        "Can we pay the entire amount of Rs 5600 all at once?\n",
        "Unfortunately no, the program follows a monthly subscription model.\n",
        "What is the validity of monthly subscription? Suppose if I pay on 15th Jan, then do I have to pay again on 1st Feb or 15th Feb\n",
        "15th Feb. The validity period is 30 days from the day you make the payment. So essentially you can join anytime you don’t have to wait for a month to end.\n",
        "What if I don’t like the course after making the payment. What is the refund policy?\n",
        "You get a 7 days refund period from the day you have made the payment.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmail.com\n",
        "Post registration queries\n",
        "Till when can I view the paid videos on the website?\n",
        "This one is tricky, so read carefully. You can watch the videos till your subscription is valid. Suppose you have purchased subscription on 21st Jan, you will be able to watch all the past paid sessions in the period of 21st Jan to 20th Feb. But after 21st Feb you will have to purchase the subscription again.\n",
        "But once the course is over and you have paid us Rs 5600(or 7 installments of Rs 799) you will be able to watch the paid sessions till Aug 2024.\n",
        "Why lifetime validity is not provided?\n",
        "Because of the low course fee.\n",
        "Where can I reach out in case of a doubt after the session?\n",
        "You will have to fill a google form provided in your dashboard and our team will contact you for a 1 on 1 doubt clearance session\n",
        "If I join the program late, can I still ask past week doubts?\n",
        "Yes, just select past week doubt in the doubt clearance google form.\n",
        "I am living outside India and I am not able to make the payment on the website, what should I do?\n",
        "You have to contact us by sending a mail at nitish.campusx@gmai.com\n",
        "Certificate and Placement Assistance related queries\n",
        "What is the criteria to get the certificate?\n",
        "There are 2 criterias:\n",
        "You have to pay the entire fee of Rs 5600\n",
        "You have to attempt all the course assessments.\n",
        "I am joining late. How can I pay payment of the earlier months?\n",
        "You will get a link to pay fee of earlier months in your dashboard once you pay for the current month.\n",
        "I have read that Placement assistance is a part of this program. What comes under Placement assistance?\n",
        "This is to clarify that Placement assistance does not mean Placement guarantee. So we dont guarantee you any jobs or for that matter even interview calls. So if you are planning to join this course just for placements, I am afraid you will be disappointed. Here is what comes under placement assistance\n",
        "Portfolio Building sessions\n",
        "Soft skill sessions\n",
        "Sessions with industry mentors\n",
        "Discussion on Job hunting strategies\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "wi0B6jtcBT7M"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on your text data\n",
        "tokenizer.fit_on_texts([faqs])\n",
        "\n",
        "#Shows mapping of words with numbers\n",
        "print(tokenizer.word_index)\n",
        "print(len(tokenizer.word_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARx7fNblBmJf",
        "outputId": "f0b050ea-3fcc-495b-df0b-b9d65e16a224"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'you': 2, 'i': 3, 'to': 4, 'a': 5, 'of': 6, 'is': 7, 'have': 8, 'will': 9, 'can': 10, 'what': 11, 'course': 12, 'program': 13, 'in': 14, 'for': 15, 'all': 16, 'sessions': 17, 'on': 18, 'be': 19, 'and': 20, 'this': 21, 'if': 22, 'am': 23, 'pay': 24, 'payment': 25, 'make': 26, 'we': 27, 'do': 28, 'subscription': 29, 'where': 30, 'rs': 31, 'so': 32, 'campusx': 33, 'session': 34, 'our': 35, 'paid': 36, 'join': 37, 'able': 38, 'your': 39, 'website': 40, 'placement': 41, 'fee': 42, 'data': 43, 'monthly': 44, 'month': 45, 'not': 46, 'get': 47, 'yes': 48, 'once': 49, 'past': 50, 'feb': 51, 'assistance': 52, 'science': 53, '7': 54, '5600': 55, 'are': 56, 'watch': 57, 'google': 58, 'by': 59, 'com': 60, 'mail': 61, 'from': 62, 'contact': 63, 'us': 64, 'at': 65, 'or': 66, 'doubt': 67, 'mentorship': 68, 'payments': 69, '799': 70, 'total': 71, 'duration': 72, 'months': 73, 'learning': 74, 'case': 75, 'here': 76, 'https': 77, 'part': 78, 'see': 79, 'late': 80, 'dashboard': 81, 'task': 82, 'don’t': 83, 'nitish': 84, 'validity': 85, '15th': 86, 'jan': 87, 'period': 88, 'after': 89, 'till': 90, '21st': 91, 'that': 92, 'about': 93, 'follows': 94, 'model': 95, 'syllabus': 96, 'python': 97, 'ml': 98, 'studies': 99, 'learnwith': 100, 'deep': 101, 'nlp': 102, 'no': 103, 'miss': 104, 'live': 105, 'recording': 106, 'even': 107, 'class': 108, 'time': 109, '2': 110, 'how': 111, 'absolutely': 112, 'middle': 113, 'anytime': 114, 'submit': 115, 'with': 116, 'gmail': 117, 'registration': 118, 'related': 119, 'link': 120, 'entire': 121, 'suppose': 122, 'again': 123, 'days': 124, 'day': 125, 'refund': 126, 'living': 127, 'outside': 128, 'india': 129, 'should': 130, 'sending': 131, 'queries': 132, 'videos': 133, 'read': 134, 'but': 135, 'provided': 136, 'form': 137, '1': 138, 'clearance': 139, 'week': 140, 'just': 141, 'certificate': 142, 'earlier': 143, 'comes': 144, 'under': 145, 'guarantee': 146, 'dsmp': 147, '2023': 148, 'becomes': 149, 'approx': 150, 'covering': 151, 'following': 152, 'modules': 153, 'fundamentals': 154, 'libraries': 155, 'analysis': 156, 'sql': 157, 'maths': 158, 'machine': 159, 'algorithms': 160, 'practical': 161, 'mlops': 162, 'check': 163, 'detailed': 164, 'courses': 165, '637339afe4b0615a1bbed390': 166, 'both': 167, 'program’s': 168, 'curriculum': 169, 'recorded': 170, 'go': 171, 'back': 172, 'find': 173, 'schedule': 174, 'checkout': 175, 'sheet': 176, 'table': 177, 'docs': 178, 'spreadsheets': 179, 'd': 180, '16ootax': 181, 'a6oraecg4emgexhqqpv3noqpyku7rj6arozk': 182, 'edit': 183, 'usp': 184, 'sharing': 185, 'roughly': 186, 'last': 187, 'hours': 188, 'language': 189, 'spoken': 190, 'instructor': 191, 'during': 192, 'hinglish': 193, 'informed': 194, 'upcoming': 195, 'side': 196, 'before': 197, 'every': 198, 'become': 199, 'user': 200, 'non': 201, 'tech': 202, 'background': 203, 'lectures': 204, 'content': 205, 'provide': 206, 'solutions': 207, 'self': 208, 'evaluate': 209, 'yourself': 210, 'questions': 211, 'youtube': 212, 'channel': 213, 'amount': 214, 'unfortunately': 215, 'then': 216, '1st': 217, '30': 218, 'essentially': 219, 'wait': 220, 'end': 221, 'like': 222, 'making': 223, 'policy': 224, 'made': 225, 'post': 226, 'when': 227, 'view': 228, 'one': 229, 'tricky': 230, 'carefully': 231, 'valid': 232, 'purchased': 233, '20th': 234, 'purchase': 235, 'over': 236, 'installments': 237, 'aug': 238, '2024': 239, 'why': 240, 'lifetime': 241, 'because': 242, 'low': 243, 'reach': 244, 'out': 245, 'fill': 246, 'team': 247, 'still': 248, 'ask': 249, 'doubts': 250, 'select': 251, 'gmai': 252, 'criteria': 253, 'there': 254, 'criterias': 255, 'attempt': 256, 'assessments': 257, 'joining': 258, 'current': 259, 'clarify': 260, 'does': 261, 'mean': 262, 'dont': 263, 'any': 264, 'jobs': 265, 'matter': 266, 'interview': 267, 'calls': 268, 'planning': 269, 'placements': 270, 'afraid': 271, 'disappointed': 272, 'portfolio': 273, 'building': 274, 'soft': 275, 'skill': 276, 'industry': 277, 'mentors': 278, 'discussion': 279, 'job': 280, 'hunting': 281, 'strategies': 282}\n",
            "282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for sentence in faqs.split('\\n'):\n",
        "  tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])\n",
        "\n",
        "print(input_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oepLQnvTyaV1",
        "outputId": "7e7f53f4-3c24-4fba-f3e6-4c2fcdb69ca1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[93, 1], [93, 1, 13], [11, 7], [11, 7, 1], [11, 7, 1, 12], [11, 7, 1, 12, 42], [11, 7, 1, 12, 42, 15], [11, 7, 1, 12, 42, 15, 43], [11, 7, 1, 12, 42, 15, 43, 53], [11, 7, 1, 12, 42, 15, 43, 53, 68], [11, 7, 1, 12, 42, 15, 43, 53, 68, 13], [11, 7, 1, 12, 42, 15, 43, 53, 68, 13, 147], [11, 7, 1, 12, 42, 15, 43, 53, 68, 13, 147, 148], [1, 12], [1, 12, 94], [1, 12, 94, 5], [1, 12, 94, 5, 44], [1, 12, 94, 5, 44, 29], [1, 12, 94, 5, 44, 29, 95], [1, 12, 94, 5, 44, 29, 95, 30], [1, 12, 94, 5, 44, 29, 95, 30, 2], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26, 44], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26, 44, 69], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26, 44, 69, 6], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26, 44, 69, 6, 31], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26, 44, 69, 6, 31, 70], [1, 12, 94, 5, 44, 29, 95, 30, 2, 8, 4, 26, 44, 69, 6, 31, 70, 45], [11, 7], [11, 7, 1], [11, 7, 1, 71], [11, 7, 1, 71, 72], [11, 7, 1, 71, 72, 6], [11, 7, 1, 71, 72, 6, 1], [11, 7, 1, 71, 72, 6, 1, 12], [1, 71], [1, 71, 72], [1, 71, 72, 6], [1, 71, 72, 6, 1], [1, 71, 72, 6, 1, 12], [1, 71, 72, 6, 1, 12, 7], [1, 71, 72, 6, 1, 12, 7, 54], [1, 71, 72, 6, 1, 12, 7, 54, 73], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42, 149], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42, 149, 70], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42, 149, 70, 54], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42, 149, 70, 54, 31], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42, 149, 70, 54, 31, 55], [1, 71, 72, 6, 1, 12, 7, 54, 73, 32, 1, 71, 12, 42, 149, 70, 54, 31, 55, 150], [11, 7], [11, 7, 1], [11, 7, 1, 96], [11, 7, 1, 96, 6], [11, 7, 1, 96, 6, 1], [11, 7, 1, 96, 6, 1, 68], [11, 7, 1, 96, 6, 1, 68, 13], [27, 9], [27, 9, 19], [27, 9, 19, 151], [27, 9, 19, 151, 1], [27, 9, 19, 151, 1, 152], [27, 9, 19, 151, 1, 152, 153], [97, 154], [97, 155], [97, 155, 15], [97, 155, 15, 43], [97, 155, 15, 43, 53], [43, 156], [157, 15], [157, 15, 43], [157, 15, 43, 53], [158, 15], [158, 15, 159], [158, 15, 159, 74], [98, 160], [161, 98], [75, 99], [2, 10], [2, 10, 163], [2, 10, 163, 1], [2, 10, 163, 1, 164], [2, 10, 163, 1, 164, 96], [2, 10, 163, 1, 164, 96, 76], [2, 10, 163, 1, 164, 96, 76, 77], [2, 10, 163, 1, 164, 96, 76, 77, 100], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165, 33], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165, 33, 43], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165, 33, 43, 53], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165, 33, 43, 53, 68], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165, 33, 43, 53, 68, 13], [2, 10, 163, 1, 164, 96, 76, 77, 100, 33, 14, 165, 33, 43, 53, 68, 13, 166], [9, 101], [9, 101, 74], [9, 101, 74, 20], [9, 101, 74, 20, 102], [9, 101, 74, 20, 102, 19], [9, 101, 74, 20, 102, 19, 5], [9, 101, 74, 20, 102, 19, 5, 78], [9, 101, 74, 20, 102, 19, 5, 78, 6], [9, 101, 74, 20, 102, 19, 5, 78, 6, 21], [9, 101, 74, 20, 102, 19, 5, 78, 6, 21, 13], [103, 102], [103, 102, 20], [103, 102, 20, 101], [103, 102, 20, 101, 74], [103, 102, 20, 101, 74, 167], [103, 102, 20, 101, 74, 167, 56], [103, 102, 20, 101, 74, 167, 56, 46], [103, 102, 20, 101, 74, 167, 56, 46, 5], [103, 102, 20, 101, 74, 167, 56, 46, 5, 78], [103, 102, 20, 101, 74, 167, 56, 46, 5, 78, 6], [103, 102, 20, 101, 74, 167, 56, 46, 5, 78, 6, 21], [103, 102, 20, 101, 74, 167, 56, 46, 5, 78, 6, 21, 168], [103, 102, 20, 101, 74, 167, 56, 46, 5, 78, 6, 21, 168, 169], [11, 22], [11, 22, 3], [11, 22, 3, 104], [11, 22, 3, 104, 5], [11, 22, 3, 104, 5, 105], [11, 22, 3, 104, 5, 105, 34], [11, 22, 3, 104, 5, 105, 34, 9], [11, 22, 3, 104, 5, 105, 34, 9, 3], [11, 22, 3, 104, 5, 105, 34, 9, 3, 47], [11, 22, 3, 104, 5, 105, 34, 9, 3, 47, 5], [11, 22, 3, 104, 5, 105, 34, 9, 3, 47, 5, 106], [11, 22, 3, 104, 5, 105, 34, 9, 3, 47, 5, 106, 6], [11, 22, 3, 104, 5, 105, 34, 9, 3, 47, 5, 106, 6, 1], [11, 22, 3, 104, 5, 105, 34, 9, 3, 47, 5, 106, 6, 1, 34], [48, 16], [48, 16, 35], [48, 16, 35, 17], [48, 16, 35, 17, 56], [48, 16, 35, 17, 56, 170], [48, 16, 35, 17, 56, 170, 32], [48, 16, 35, 17, 56, 170, 32, 107], [48, 16, 35, 17, 56, 170, 32, 107, 22], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10, 171], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10, 171, 172], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10, 171, 172, 20], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10, 171, 172, 20, 57], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10, 171, 172, 20, 57, 1], [48, 16, 35, 17, 56, 170, 32, 107, 22, 2, 104, 5, 34, 2, 10, 171, 172, 20, 57, 1, 106], [30, 10], [30, 10, 3], [30, 10, 3, 173], [30, 10, 3, 173, 1], [30, 10, 3, 173, 1, 108], [30, 10, 3, 173, 1, 108, 174], [175, 21], [175, 21, 58], [175, 21, 58, 176], [175, 21, 58, 176, 4], [175, 21, 58, 176, 4, 79], [175, 21, 58, 176, 4, 79, 45], [175, 21, 58, 176, 4, 79, 45, 59], [175, 21, 58, 176, 4, 79, 45, 59, 45], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179, 180], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179, 180, 181], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179, 180, 181, 182], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179, 180, 181, 182, 183], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179, 180, 181, 182, 183, 184], [175, 21, 58, 176, 4, 79, 45, 59, 45, 109, 177, 6, 1, 12, 77, 178, 58, 60, 179, 180, 181, 182, 183, 184, 185], [11, 7], [11, 7, 1], [11, 7, 1, 109], [11, 7, 1, 109, 72], [11, 7, 1, 109, 72, 6], [11, 7, 1, 109, 72, 6, 16], [11, 7, 1, 109, 72, 6, 16, 1], [11, 7, 1, 109, 72, 6, 16, 1, 105], [11, 7, 1, 109, 72, 6, 16, 1, 105, 17], [186, 16], [186, 16, 1], [186, 16, 1, 17], [186, 16, 1, 17, 187], [186, 16, 1, 17, 187, 110], [186, 16, 1, 17, 187, 110, 188], [11, 7], [11, 7, 1], [11, 7, 1, 189], [11, 7, 1, 189, 190], [11, 7, 1, 189, 190, 59], [11, 7, 1, 189, 190, 59, 1], [11, 7, 1, 189, 190, 59, 1, 191], [11, 7, 1, 189, 190, 59, 1, 191, 192], [11, 7, 1, 189, 190, 59, 1, 191, 192, 1], [11, 7, 1, 189, 190, 59, 1, 191, 192, 1, 17], [111, 9], [111, 9, 3], [111, 9, 3, 19], [111, 9, 3, 19, 194], [111, 9, 3, 19, 194, 93], [111, 9, 3, 19, 194, 93, 1], [111, 9, 3, 19, 194, 93, 1, 195], [111, 9, 3, 19, 194, 93, 1, 195, 108], [2, 9], [2, 9, 47], [2, 9, 47, 5], [2, 9, 47, 5, 61], [2, 9, 47, 5, 61, 62], [2, 9, 47, 5, 61, 62, 35], [2, 9, 47, 5, 61, 62, 35, 196], [2, 9, 47, 5, 61, 62, 35, 196, 197], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34, 49], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34, 49, 2], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34, 49, 2, 199], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34, 49, 2, 199, 5], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34, 49, 2, 199, 5, 36], [2, 9, 47, 5, 61, 62, 35, 196, 197, 198, 36, 34, 49, 2, 199, 5, 36, 200], [10, 3], [10, 3, 28], [10, 3, 28, 21], [10, 3, 28, 21, 12], [10, 3, 28, 21, 12, 22], [10, 3, 28, 21, 12, 22, 3], [10, 3, 28, 21, 12, 22, 3, 23], [10, 3, 28, 21, 12, 22, 3, 23, 62], [10, 3, 28, 21, 12, 22, 3, 23, 62, 5], [10, 3, 28, 21, 12, 22, 3, 23, 62, 5, 201], [10, 3, 28, 21, 12, 22, 3, 23, 62, 5, 201, 202], [10, 3, 28, 21, 12, 22, 3, 23, 62, 5, 201, 202, 203], [48, 112], [3, 23], [3, 23, 80], [3, 23, 80, 10], [3, 23, 80, 10, 3], [3, 23, 80, 10, 3, 37], [3, 23, 80, 10, 3, 37, 1], [3, 23, 80, 10, 3, 37, 1, 13], [3, 23, 80, 10, 3, 37, 1, 13, 14], [3, 23, 80, 10, 3, 37, 1, 13, 14, 1], [3, 23, 80, 10, 3, 37, 1, 13, 14, 1, 113], [112, 2], [112, 2, 10], [112, 2, 10, 37], [112, 2, 10, 37, 1], [112, 2, 10, 37, 1, 13], [112, 2, 10, 37, 1, 13, 114], [22, 3], [22, 3, 37], [22, 3, 37, 24], [22, 3, 37, 24, 14], [22, 3, 37, 24, 14, 1], [22, 3, 37, 24, 14, 1, 113], [22, 3, 37, 24, 14, 1, 113, 9], [22, 3, 37, 24, 14, 1, 113, 9, 3], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38, 4], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38, 4, 79], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38, 4, 79, 16], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38, 4, 79, 16, 1], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38, 4, 79, 16, 1, 50], [22, 3, 37, 24, 14, 1, 113, 9, 3, 19, 38, 4, 79, 16, 1, 50, 204], [48, 49], [48, 49, 2], [48, 49, 2, 26], [48, 49, 2, 26, 1], [48, 49, 2, 26, 1, 25], [48, 49, 2, 26, 1, 25, 2], [48, 49, 2, 26, 1, 25, 2, 9], [48, 49, 2, 26, 1, 25, 2, 9, 19], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16, 1], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16, 1, 50], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16, 1, 50, 205], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16, 1, 50, 205, 14], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16, 1, 50, 205, 14, 39], [48, 49, 2, 26, 1, 25, 2, 9, 19, 38, 4, 79, 16, 1, 50, 205, 14, 39, 81], [30, 28], [30, 28, 3], [30, 28, 3, 8], [30, 28, 3, 8, 4], [30, 28, 3, 8, 4, 115], [30, 28, 3, 8, 4, 115, 1], [30, 28, 3, 8, 4, 115, 1, 82], [2, 83], [2, 83, 8], [2, 83, 8, 4], [2, 83, 8, 4, 115], [2, 83, 8, 4, 115, 1], [2, 83, 8, 4, 115, 1, 82], [2, 83, 8, 4, 115, 1, 82, 27], [2, 83, 8, 4, 115, 1, 82, 27, 9], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8, 4], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8, 4, 208], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8, 4, 208, 209], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8, 4, 208, 209, 1], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8, 4, 208, 209, 1, 82], [2, 83, 8, 4, 115, 1, 82, 27, 9, 206, 2, 116, 1, 207, 2, 8, 4, 208, 209, 1, 82, 210], [9, 27], [9, 27, 28], [9, 27, 28, 75], [9, 27, 28, 75, 99], [9, 27, 28, 75, 99, 14], [9, 27, 28, 75, 99, 14, 1], [9, 27, 28, 75, 99, 14, 1, 13], [30, 10], [30, 10, 27], [30, 10, 27, 63], [30, 10, 27, 63, 2], [2, 10], [2, 10, 61], [2, 10, 61, 64], [2, 10, 61, 64, 65], [2, 10, 61, 64, 65, 84], [2, 10, 61, 64, 65, 84, 33], [2, 10, 61, 64, 65, 84, 33, 117], [2, 10, 61, 64, 65, 84, 33, 117, 60], [25, 118], [25, 118, 119], [25, 118, 119, 211], [30, 28], [30, 28, 27], [30, 28, 27, 8], [30, 28, 27, 8, 4], [30, 28, 27, 8, 4, 26], [30, 28, 27, 8, 4, 26, 35], [30, 28, 27, 8, 4, 26, 35, 69], [30, 28, 27, 8, 4, 26, 35, 69, 39], [30, 28, 27, 8, 4, 26, 35, 69, 39, 212], [30, 28, 27, 8, 4, 26, 35, 69, 39, 212, 213], [30, 28, 27, 8, 4, 26, 35, 69, 39, 212, 213, 66], [30, 28, 27, 8, 4, 26, 35, 69, 39, 212, 213, 66, 40], [2, 8], [2, 8, 4], [2, 8, 4, 26], [2, 8, 4, 26, 16], [2, 8, 4, 26, 16, 39], [2, 8, 4, 26, 16, 39, 44], [2, 8, 4, 26, 16, 39, 44, 69], [2, 8, 4, 26, 16, 39, 44, 69, 18], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15, 35], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15, 35, 40], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15, 35, 40, 77], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15, 35, 40, 77, 100], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15, 35, 40, 77, 100, 33], [2, 8, 4, 26, 16, 39, 44, 69, 18, 35, 40, 76, 7, 1, 120, 15, 35, 40, 77, 100, 33, 14], [10, 27], [10, 27, 24], [10, 27, 24, 1], [10, 27, 24, 1, 121], [10, 27, 24, 1, 121, 214], [10, 27, 24, 1, 121, 214, 6], [10, 27, 24, 1, 121, 214, 6, 31], [10, 27, 24, 1, 121, 214, 6, 31, 55], [10, 27, 24, 1, 121, 214, 6, 31, 55, 16], [10, 27, 24, 1, 121, 214, 6, 31, 55, 16, 65], [10, 27, 24, 1, 121, 214, 6, 31, 55, 16, 65, 49], [215, 103], [215, 103, 1], [215, 103, 1, 13], [215, 103, 1, 13, 94], [215, 103, 1, 13, 94, 5], [215, 103, 1, 13, 94, 5, 44], [215, 103, 1, 13, 94, 5, 44, 29], [215, 103, 1, 13, 94, 5, 44, 29, 95], [11, 7], [11, 7, 1], [11, 7, 1, 85], [11, 7, 1, 85, 6], [11, 7, 1, 85, 6, 44], [11, 7, 1, 85, 6, 44, 29], [11, 7, 1, 85, 6, 44, 29, 122], [11, 7, 1, 85, 6, 44, 29, 122, 22], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123, 18], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123, 18, 217], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123, 18, 217, 51], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123, 18, 217, 51, 66], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123, 18, 217, 51, 66, 86], [11, 7, 1, 85, 6, 44, 29, 122, 22, 3, 24, 18, 86, 87, 216, 28, 3, 8, 4, 24, 123, 18, 217, 51, 66, 86, 51], [86, 51], [86, 51, 1], [86, 51, 1, 85], [86, 51, 1, 85, 88], [86, 51, 1, 85, 88, 7], [86, 51, 1, 85, 88, 7, 218], [86, 51, 1, 85, 88, 7, 218, 124], [86, 51, 1, 85, 88, 7, 218, 124, 62], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4, 220], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4, 220, 15], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4, 220, 15, 5], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4, 220, 15, 5, 45], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4, 220, 15, 5, 45, 4], [86, 51, 1, 85, 88, 7, 218, 124, 62, 1, 125, 2, 26, 1, 25, 32, 219, 2, 10, 37, 114, 2, 83, 8, 4, 220, 15, 5, 45, 4, 221], [11, 22], [11, 22, 3], [11, 22, 3, 83], [11, 22, 3, 83, 222], [11, 22, 3, 83, 222, 1], [11, 22, 3, 83, 222, 1, 12], [11, 22, 3, 83, 222, 1, 12, 89], [11, 22, 3, 83, 222, 1, 12, 89, 223], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1, 25], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1, 25, 11], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1, 25, 11, 7], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1, 25, 11, 7, 1], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1, 25, 11, 7, 1, 126], [11, 22, 3, 83, 222, 1, 12, 89, 223, 1, 25, 11, 7, 1, 126, 224], [2, 47], [2, 47, 5], [2, 47, 5, 54], [2, 47, 5, 54, 124], [2, 47, 5, 54, 124, 126], [2, 47, 5, 54, 124, 126, 88], [2, 47, 5, 54, 124, 126, 88, 62], [2, 47, 5, 54, 124, 126, 88, 62, 1], [2, 47, 5, 54, 124, 126, 88, 62, 1, 125], [2, 47, 5, 54, 124, 126, 88, 62, 1, 125, 2], [2, 47, 5, 54, 124, 126, 88, 62, 1, 125, 2, 8], [2, 47, 5, 54, 124, 126, 88, 62, 1, 125, 2, 8, 225], [2, 47, 5, 54, 124, 126, 88, 62, 1, 125, 2, 8, 225, 1], [2, 47, 5, 54, 124, 126, 88, 62, 1, 125, 2, 8, 225, 1, 25], [3, 23], [3, 23, 127], [3, 23, 127, 128], [3, 23, 127, 128, 129], [3, 23, 127, 128, 129, 20], [3, 23, 127, 128, 129, 20, 3], [3, 23, 127, 128, 129, 20, 3, 23], [3, 23, 127, 128, 129, 20, 3, 23, 46], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11, 130], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11, 130, 3], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11, 130, 3, 28], [2, 8], [2, 8, 4], [2, 8, 4, 63], [2, 8, 4, 63, 64], [2, 8, 4, 63, 64, 59], [2, 8, 4, 63, 64, 59, 131], [2, 8, 4, 63, 64, 59, 131, 5], [2, 8, 4, 63, 64, 59, 131, 5, 61], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84, 33], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84, 33, 117], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84, 33, 117, 60], [226, 118], [226, 118, 132], [90, 227], [90, 227, 10], [90, 227, 10, 3], [90, 227, 10, 3, 228], [90, 227, 10, 3, 228, 1], [90, 227, 10, 3, 228, 1, 36], [90, 227, 10, 3, 228, 1, 36, 133], [90, 227, 10, 3, 228, 1, 36, 133, 18], [90, 227, 10, 3, 228, 1, 36, 133, 18, 1], [90, 227, 10, 3, 228, 1, 36, 133, 18, 1, 40], [21, 229], [21, 229, 7], [21, 229, 7, 230], [21, 229, 7, 230, 32], [21, 229, 7, 230, 32, 134], [21, 229, 7, 230, 32, 134, 231], [21, 229, 7, 230, 32, 134, 231, 2], [21, 229, 7, 230, 32, 134, 231, 2, 10], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9, 8], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9, 8, 4], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9, 8, 4, 235], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9, 8, 4, 235, 1], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9, 8, 4, 235, 1, 29], [21, 229, 7, 230, 32, 134, 231, 2, 10, 57, 1, 133, 90, 39, 29, 7, 232, 122, 2, 8, 233, 29, 18, 91, 87, 2, 9, 19, 38, 4, 57, 16, 1, 50, 36, 17, 14, 1, 88, 6, 91, 87, 4, 234, 51, 135, 89, 91, 51, 2, 9, 8, 4, 235, 1, 29, 123], [135, 49], [135, 49, 1], [135, 49, 1, 12], [135, 49, 1, 12, 7], [135, 49, 1, 12, 7, 236], [135, 49, 1, 12, 7, 236, 20], [135, 49, 1, 12, 7, 236, 20, 2], [135, 49, 1, 12, 7, 236, 20, 2, 8], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57, 1], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57, 1, 36], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57, 1, 36, 17], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57, 1, 36, 17, 90], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57, 1, 36, 17, 90, 238], [135, 49, 1, 12, 7, 236, 20, 2, 8, 36, 64, 31, 55, 66, 54, 237, 6, 31, 70, 2, 9, 19, 38, 4, 57, 1, 36, 17, 90, 238, 239], [240, 241], [240, 241, 85], [240, 241, 85, 7], [240, 241, 85, 7, 46], [240, 241, 85, 7, 46, 136], [242, 6], [242, 6, 1], [242, 6, 1, 243], [242, 6, 1, 243, 12], [242, 6, 1, 243, 12, 42], [30, 10], [30, 10, 3], [30, 10, 3, 244], [30, 10, 3, 244, 245], [30, 10, 3, 244, 245, 14], [30, 10, 3, 244, 245, 14, 75], [30, 10, 3, 244, 245, 14, 75, 6], [30, 10, 3, 244, 245, 14, 75, 6, 5], [30, 10, 3, 244, 245, 14, 75, 6, 5, 67], [30, 10, 3, 244, 245, 14, 75, 6, 5, 67, 89], [30, 10, 3, 244, 245, 14, 75, 6, 5, 67, 89, 1], [30, 10, 3, 244, 245, 14, 75, 6, 5, 67, 89, 1, 34], [2, 9], [2, 9, 8], [2, 9, 8, 4], [2, 9, 8, 4, 246], [2, 9, 8, 4, 246, 5], [2, 9, 8, 4, 246, 5, 58], [2, 9, 8, 4, 246, 5, 58, 137], [2, 9, 8, 4, 246, 5, 58, 137, 136], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5, 138], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5, 138, 18], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5, 138, 18, 138], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5, 138, 18, 138, 67], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5, 138, 18, 138, 67, 139], [2, 9, 8, 4, 246, 5, 58, 137, 136, 14, 39, 81, 20, 35, 247, 9, 63, 2, 15, 5, 138, 18, 138, 67, 139, 34], [22, 3], [22, 3, 37], [22, 3, 37, 1], [22, 3, 37, 1, 13], [22, 3, 37, 1, 13, 80], [22, 3, 37, 1, 13, 80, 10], [22, 3, 37, 1, 13, 80, 10, 3], [22, 3, 37, 1, 13, 80, 10, 3, 248], [22, 3, 37, 1, 13, 80, 10, 3, 248, 249], [22, 3, 37, 1, 13, 80, 10, 3, 248, 249, 50], [22, 3, 37, 1, 13, 80, 10, 3, 248, 249, 50, 140], [22, 3, 37, 1, 13, 80, 10, 3, 248, 249, 50, 140, 250], [48, 141], [48, 141, 251], [48, 141, 251, 50], [48, 141, 251, 50, 140], [48, 141, 251, 50, 140, 67], [48, 141, 251, 50, 140, 67, 14], [48, 141, 251, 50, 140, 67, 14, 1], [48, 141, 251, 50, 140, 67, 14, 1, 67], [48, 141, 251, 50, 140, 67, 14, 1, 67, 139], [48, 141, 251, 50, 140, 67, 14, 1, 67, 139, 58], [48, 141, 251, 50, 140, 67, 14, 1, 67, 139, 58, 137], [3, 23], [3, 23, 127], [3, 23, 127, 128], [3, 23, 127, 128, 129], [3, 23, 127, 128, 129, 20], [3, 23, 127, 128, 129, 20, 3], [3, 23, 127, 128, 129, 20, 3, 23], [3, 23, 127, 128, 129, 20, 3, 23, 46], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11, 130], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11, 130, 3], [3, 23, 127, 128, 129, 20, 3, 23, 46, 38, 4, 26, 1, 25, 18, 1, 40, 11, 130, 3, 28], [2, 8], [2, 8, 4], [2, 8, 4, 63], [2, 8, 4, 63, 64], [2, 8, 4, 63, 64, 59], [2, 8, 4, 63, 64, 59, 131], [2, 8, 4, 63, 64, 59, 131, 5], [2, 8, 4, 63, 64, 59, 131, 5, 61], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84, 33], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84, 33, 252], [2, 8, 4, 63, 64, 59, 131, 5, 61, 65, 84, 33, 252, 60], [142, 20], [142, 20, 41], [142, 20, 41, 52], [142, 20, 41, 52, 119], [142, 20, 41, 52, 119, 132], [11, 7], [11, 7, 1], [11, 7, 1, 253], [11, 7, 1, 253, 4], [11, 7, 1, 253, 4, 47], [11, 7, 1, 253, 4, 47, 1], [11, 7, 1, 253, 4, 47, 1, 142], [254, 56], [254, 56, 110], [254, 56, 110, 255], [2, 8], [2, 8, 4], [2, 8, 4, 24], [2, 8, 4, 24, 1], [2, 8, 4, 24, 1, 121], [2, 8, 4, 24, 1, 121, 42], [2, 8, 4, 24, 1, 121, 42, 6], [2, 8, 4, 24, 1, 121, 42, 6, 31], [2, 8, 4, 24, 1, 121, 42, 6, 31, 55], [2, 8], [2, 8, 4], [2, 8, 4, 256], [2, 8, 4, 256, 16], [2, 8, 4, 256, 16, 1], [2, 8, 4, 256, 16, 1, 12], [2, 8, 4, 256, 16, 1, 12, 257], [3, 23], [3, 23, 258], [3, 23, 258, 80], [3, 23, 258, 80, 111], [3, 23, 258, 80, 111, 10], [3, 23, 258, 80, 111, 10, 3], [3, 23, 258, 80, 111, 10, 3, 24], [3, 23, 258, 80, 111, 10, 3, 24, 25], [3, 23, 258, 80, 111, 10, 3, 24, 25, 6], [3, 23, 258, 80, 111, 10, 3, 24, 25, 6, 1], [3, 23, 258, 80, 111, 10, 3, 24, 25, 6, 1, 143], [3, 23, 258, 80, 111, 10, 3, 24, 25, 6, 1, 143, 73], [2, 9], [2, 9, 47], [2, 9, 47, 5], [2, 9, 47, 5, 120], [2, 9, 47, 5, 120, 4], [2, 9, 47, 5, 120, 4, 24], [2, 9, 47, 5, 120, 4, 24, 42], [2, 9, 47, 5, 120, 4, 24, 42, 6], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49, 2], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49, 2, 24], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49, 2, 24, 15], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49, 2, 24, 15, 1], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49, 2, 24, 15, 1, 259], [2, 9, 47, 5, 120, 4, 24, 42, 6, 143, 73, 14, 39, 81, 49, 2, 24, 15, 1, 259, 45], [3, 8], [3, 8, 134], [3, 8, 134, 92], [3, 8, 134, 92, 41], [3, 8, 134, 92, 41, 52], [3, 8, 134, 92, 41, 52, 7], [3, 8, 134, 92, 41, 52, 7, 5], [3, 8, 134, 92, 41, 52, 7, 5, 78], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21, 13], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21, 13, 11], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21, 13, 11, 144], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21, 13, 11, 144, 145], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21, 13, 11, 144, 145, 41], [3, 8, 134, 92, 41, 52, 7, 5, 78, 6, 21, 13, 11, 144, 145, 41, 52], [21, 7], [21, 7, 4], [21, 7, 4, 260], [21, 7, 4, 260, 92], [21, 7, 4, 260, 92, 41], [21, 7, 4, 260, 92, 41, 52], [21, 7, 4, 260, 92, 41, 52, 261], [21, 7, 4, 260, 92, 41, 52, 261, 46], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76, 7], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76, 7, 11], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76, 7, 11, 144], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76, 7, 11, 144, 145], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76, 7, 11, 144, 145, 41], [21, 7, 4, 260, 92, 41, 52, 261, 46, 262, 41, 146, 32, 27, 263, 146, 2, 264, 265, 66, 15, 92, 266, 107, 267, 268, 32, 22, 2, 56, 269, 4, 37, 21, 12, 141, 15, 270, 3, 23, 271, 2, 9, 19, 272, 76, 7, 11, 144, 145, 41, 52], [273, 274], [273, 274, 17], [275, 276], [275, 276, 17], [17, 116], [17, 116, 277], [17, 116, 277, 278], [279, 18], [279, 18, 280], [279, 18, 280, 281], [279, 18, 280, 281, 282]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(i) for i in input_sequences])\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tesI5saz7L8",
        "outputId": "4172f888-5e17-45cf-f075-671f608838f7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen = max_len, padding='pre')\n",
        "\n",
        "padded_input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC3dYlFe0Lcp",
        "outputId": "27e50d81-3247-4759-b5c2-9dcfd6379000"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,  93,   1],\n",
              "       [  0,   0,   0, ...,  93,   1,  13],\n",
              "       [  0,   0,   0, ...,   0,  11,   7],\n",
              "       ...,\n",
              "       [  0,   0,   0, ..., 279,  18, 280],\n",
              "       [  0,   0,   0, ...,  18, 280, 281],\n",
              "       [  0,   0,   0, ..., 280, 281, 282]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = padded_input_sequences[:,:-1]\n",
        "y= padded_input_sequences[:,-1]"
      ],
      "metadata": {
        "id": "7HLi34BG0fBB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGW8_vmQ0nFC",
        "outputId": "87affeef-e09d-42cb-e1af-53a6c072c988"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(863, 56)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[2:7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9LA_rVU3s-7",
        "outputId": "5ff102fe-679d-4297-f040-9e6199f18b9b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0, 11],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0, 11,  7],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0, 11,  7,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0, 11,  7,  1, 12],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0, 11,  7,  1, 12, 42]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dMEFdh60o2E",
        "outputId": "1138f099-fca3-425c-cdc0-18b9afd762c5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(863,)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This means that the function will convert your class labels (y) into one-hot encoded vectors, and each vector will have a length of 283.\n",
        "# Each class will be represented as a unique combination of 283 binary values (0 or 1).\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,num_classes=283)"
      ],
      "metadata": {
        "id": "vUiwtanA2r2F"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-R5prOx26bZ",
        "outputId": "51aa8ea8-f4a0-4e1e-a2a2-21a208d619e1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(863, 283)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XbEjRG328Er",
        "outputId": "b439bbd3-59c5-4583-d8db-9dc592acac79"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "metadata": {
        "id": "RzVwFFad3MyD"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the embedding layer:\n",
        "First Argument (283):\n",
        "\n",
        "The first argument in the Embedding layer represents the size of the vocabulary or the total number of unique classes in your dataset. In your case, it's set to 283.\n",
        "This parameter determines the size of the one-hot encoding for each input sample. Each input sample, which is a class label in this case, will be represented as a one-hot encoded vector of length 283.\n",
        "input_length Parameter:\n",
        "\n",
        "The second argument (100) is the output dimension, which represents the size of the embedding vector for each word or class.\n",
        "\n",
        "The input_length parameter is used to specify the length of the input sequences. In the context of an Embedding layer, it indicates the number of time steps in each input sequence.\n",
        "In our code, input_length is set to 56, meaning that each input sequence (representing a sample or class label) is expected to have a length of 56."
      ],
      "metadata": {
        "id": "asl4h48z4uNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(283, 100, input_length=56))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(283, activation='softmax'))"
      ],
      "metadata": {
        "id": "yGXdCXv13Wuq"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "D5FcCr7t5XY-"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x96jw-ur5h3B",
        "outputId": "11d94740-dfe7-44f6-d4cc-71833851170a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 56, 100)           28300     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 283)               42733     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 221633 (865.75 KB)\n",
            "Trainable params: 221633 (865.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vdXgqf871fJ",
        "outputId": "b29072ee-9f84-4e0f-8b12-38ce94c69e24"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "27/27 [==============================] - 5s 87ms/step - loss: 5.4633 - accuracy: 0.0753\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 3s 127ms/step - loss: 5.0980 - accuracy: 0.0776\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 5.0164 - accuracy: 0.0776\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 4.9891 - accuracy: 0.0776\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 4.9525 - accuracy: 0.0776\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 4.8707 - accuracy: 0.0788\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 3s 126ms/step - loss: 4.7239 - accuracy: 0.0892\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 4.5058 - accuracy: 0.1286\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 4.2601 - accuracy: 0.1715\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 4.0032 - accuracy: 0.2097\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 3s 130ms/step - loss: 3.7567 - accuracy: 0.2387\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 7s 242ms/step - loss: 3.5112 - accuracy: 0.2723\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 3.2861 - accuracy: 0.3094\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 3.0643 - accuracy: 0.3453\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 3s 130ms/step - loss: 2.8641 - accuracy: 0.3824\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 2.6723 - accuracy: 0.4195\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 2.4968 - accuracy: 0.4415\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 2.3449 - accuracy: 0.4670\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 3s 98ms/step - loss: 2.1777 - accuracy: 0.5075\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 3s 116ms/step - loss: 2.0299 - accuracy: 0.5388\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 1.8894 - accuracy: 0.5747\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 1.7603 - accuracy: 0.6222\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 1.6430 - accuracy: 0.6663\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 4s 131ms/step - loss: 1.5350 - accuracy: 0.6929\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 1.4368 - accuracy: 0.7115\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 1.3353 - accuracy: 0.7393\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 1.2479 - accuracy: 0.7613\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 3s 93ms/step - loss: 1.1617 - accuracy: 0.7798\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 4s 166ms/step - loss: 1.0900 - accuracy: 0.8053\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 3s 96ms/step - loss: 1.0219 - accuracy: 0.8262\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.9642 - accuracy: 0.8297\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.9008 - accuracy: 0.8644\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 4s 131ms/step - loss: 0.8432 - accuracy: 0.8714\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.7938 - accuracy: 0.8795\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.7393 - accuracy: 0.8934\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 0.6968 - accuracy: 0.8980\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 2s 93ms/step - loss: 0.6590 - accuracy: 0.8957\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 3s 121ms/step - loss: 0.6189 - accuracy: 0.9108\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.5800 - accuracy: 0.9270\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.5529 - accuracy: 0.9247\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 0.5242 - accuracy: 0.9293\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 3s 116ms/step - loss: 0.4976 - accuracy: 0.9305\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 3s 96ms/step - loss: 0.4731 - accuracy: 0.9351\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 0.4481 - accuracy: 0.9386\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 0.4285 - accuracy: 0.9386\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.4086 - accuracy: 0.9432\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 3s 128ms/step - loss: 0.3910 - accuracy: 0.9421\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.3732 - accuracy: 0.9444\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.3572 - accuracy: 0.9397\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.3426 - accuracy: 0.9432\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 3s 101ms/step - loss: 0.3293 - accuracy: 0.9455\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 3s 112ms/step - loss: 0.3150 - accuracy: 0.9421\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.3044 - accuracy: 0.9455\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.2951 - accuracy: 0.9386\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 0.2881 - accuracy: 0.9490\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 3s 130ms/step - loss: 0.2772 - accuracy: 0.9409\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.2661 - accuracy: 0.9467\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.2607 - accuracy: 0.9421\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.2524 - accuracy: 0.9421\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 0.2430 - accuracy: 0.9444\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 3s 126ms/step - loss: 0.2367 - accuracy: 0.9502\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.2317 - accuracy: 0.9432\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.2242 - accuracy: 0.9455\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 0.2187 - accuracy: 0.9444\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 4s 161ms/step - loss: 0.2132 - accuracy: 0.9490\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 3s 98ms/step - loss: 0.2102 - accuracy: 0.9455\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 0.2055 - accuracy: 0.9455\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.2015 - accuracy: 0.9467\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.1976 - accuracy: 0.9467\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 4s 133ms/step - loss: 0.1949 - accuracy: 0.9444\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.1894 - accuracy: 0.9467\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 0.1862 - accuracy: 0.9444\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.1841 - accuracy: 0.9444\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 3s 107ms/step - loss: 0.1796 - accuracy: 0.9455\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 3s 111ms/step - loss: 0.1758 - accuracy: 0.9467\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.1787 - accuracy: 0.9467\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.1720 - accuracy: 0.9432\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.1698 - accuracy: 0.9479\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 3s 131ms/step - loss: 0.1678 - accuracy: 0.9432\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 3s 129ms/step - loss: 0.1641 - accuracy: 0.9432\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 2s 91ms/step - loss: 0.1636 - accuracy: 0.9467\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.1601 - accuracy: 0.9444\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 3s 130ms/step - loss: 0.1590 - accuracy: 0.9490\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.1567 - accuracy: 0.9432\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.1551 - accuracy: 0.9490\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 2s 88ms/step - loss: 0.1523 - accuracy: 0.9479\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.1505 - accuracy: 0.9467\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 4s 131ms/step - loss: 0.1516 - accuracy: 0.9455\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.1491 - accuracy: 0.9467\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 3s 93ms/step - loss: 0.1449 - accuracy: 0.9479\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.1460 - accuracy: 0.9455\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 3s 110ms/step - loss: 0.1420 - accuracy: 0.9455\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 3s 105ms/step - loss: 0.1424 - accuracy: 0.9455\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 0.1425 - accuracy: 0.9467\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 2s 92ms/step - loss: 0.1412 - accuracy: 0.9397\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.1384 - accuracy: 0.9490\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 3s 130ms/step - loss: 0.1384 - accuracy: 0.9490\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 2s 87ms/step - loss: 0.1390 - accuracy: 0.9467\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 2s 90ms/step - loss: 0.1369 - accuracy: 0.9479\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 2s 89ms/step - loss: 0.1382 - accuracy: 0.9455\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79193e8406a0>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "text = \"what is the fee\"\n",
        "\n",
        "for i in range(10):\n",
        "  # tokenize\n",
        "  token_text = tokenizer.texts_to_sequences([text])[0]\n",
        "  # padding\n",
        "  padded_token_text = pad_sequences([token_text], maxlen=56, padding='pre')\n",
        "  # predict\n",
        "  pos = np.argmax(model.predict(padded_token_text))\n",
        "\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == pos:\n",
        "      text = text + \" \" + word\n",
        "      print(text)\n",
        "      time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcFx8JDP9P1o",
        "outputId": "cb6f44b1-fc45-41a8-b8b6-edb543be88c3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 470ms/step\n",
            "what is the fee of\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "what is the fee of the\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "what is the fee of the mentorship\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "what is the fee of the mentorship program\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "what is the fee of the mentorship program dsmp\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "what is the fee of the mentorship program dsmp 2023\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "what is the fee of the mentorship program dsmp 2023 7\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "what is the fee of the mentorship program dsmp 2023 7 rs\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "what is the fee of the mentorship program dsmp 2023 7 rs 5600\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "what is the fee of the mentorship program dsmp 2023 7 rs 5600 approx\n"
          ]
        }
      ]
    }
  ]
}